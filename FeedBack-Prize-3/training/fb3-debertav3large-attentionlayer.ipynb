{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install coolname","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:47:45.461083Z","iopub.execute_input":"2022-11-27T19:47:45.461635Z","iopub.status.idle":"2022-11-27T19:47:57.013246Z","shell.execute_reply.started":"2022-11-27T19:47:45.461518Z","shell.execute_reply":"2022-11-27T19:47:57.007310Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting coolname\n  Downloading coolname-2.0.0-py2.py3-none-any.whl (37 kB)\nInstalling collected packages: coolname\nSuccessfully installed coolname-2.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\nimport joblib\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utils|\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom feedback_custom_funtions import loss_fn, optimizer_setup, FeedBackDataset, RMSELoss, compute_metrics\nfrom model_building import MeanPooling, MaxPooling, MinPooling, AttentionPooling, FeedBackModel\nfrom coolname import generate_slug\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import DataCollatorWithPadding\nfrom transformers import Trainer, TrainingArguments\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nos.makedirs(\"/kaggle/tmp/\", exist_ok=True) ","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:47:57.016194Z","iopub.execute_input":"2022-11-27T19:47:57.016608Z","iopub.status.idle":"2022-11-27T19:48:04.628932Z","shell.execute_reply.started":"2022-11-27T19:47:57.016571Z","shell.execute_reply":"2022-11-27T19:48:04.627865Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Training Confg","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nhash_name = generate_slug(3)\n\nconfig = {\"seed\": 42,\n          \"epochs\": 5,\n          \"debug\" : False,\n          \"model_name\": \"microsoft/deberta-v3-large\",\n          \"PoolingLayer\": AttentionPooling(1024),\n          \"group\" : \"deberta-v3-Large-AP-LLRD\" ,\n          \"loss_type\": \"smooth_l1\", # ['mse', 'rmse', 'smooth_l1']\n          \"train_batch_size\": 4,\n          \"valid_batch_size\": 8,\n          \"fp16_enable\": True,\n          \"max_length\": 512,\n          \"layerwise\" : True,\n          \"learning_rate\": 1e-5,\n          \"decoder_lr\": 1e-4,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 4,\n          \"n_accumulate\": 4,\n          \"max_grad_norm\": 1000,\n          \"num_classes\": 6,\n          \"target_cols\": [\"cohesion\", \"syntax\", \"vocabulary\", \n                          \"phraseology\", \"grammar\", \"conventions\"],\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          \"hash_name\": hash_name,\n          \"competition\": \"FeedBack3\",\n          \"_wandb_kernel\": \"hazrul\"\n          }\n\nset_seed(config['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:48:04.630509Z","iopub.execute_input":"2022-11-27T19:48:04.631598Z","iopub.status.idle":"2022-11-27T19:48:04.713614Z","shell.execute_reply.started":"2022-11-27T19:48:04.631559Z","shell.execute_reply":"2022-11-27T19:48:04.712718Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if not config[\"debug\"]:    \n    import wandb\n\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n        wandb.login(key=api_key)\n        anony = None\n        print(\"wandb Logged in Successfully\")\n    except:\n        anony = \"must\"\n        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\nelse:\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n    print(\"Debugging...\")","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:48:04.716242Z","iopub.execute_input":"2022-11-27T19:48:04.716613Z","iopub.status.idle":"2022-11-27T19:48:06.224731Z","shell.execute_reply.started":"2022-11-27T19:48:04.716578Z","shell.execute_reply":"2022-11-27T19:48:06.223689Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"wandb Logged in Successfully\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/feedbackprizemultilabelstratifiedkfold/kfold_train_FB_comptetion.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:48:06.225920Z","iopub.execute_input":"2022-11-27T19:48:06.226197Z","iopub.status.idle":"2022-11-27T19:48:06.453158Z","shell.execute_reply.started":"2022-11-27T19:48:06.226172Z","shell.execute_reply":"2022-11-27T19:48:06.452275Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        text_id                                          full_text  cohesion  \\\n0  0016926B079C  I think that students would benefit from learn...       3.5   \n1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n3  003885A45F42  The best time in life is when you become yours...       4.5   \n4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n\n   syntax  vocabulary  phraseology  grammar  conventions  kfold  \n0     3.5         3.0          3.0      4.0          3.0      1  \n1     2.5         3.0          2.0      2.0          2.5      3  \n2     3.5         3.0          3.0      3.0          2.5      2  \n3     4.5         4.5          4.5      4.0          5.0      3  \n4     3.0         3.0          3.0      2.5          2.5      3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>I think that students would benefit from learn...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>When a problem is a change you have to let it ...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>The best time in life is when you become yours...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>Small act of kindness can impact in other peop...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\nconfig[\"tokenizer\"] = tokenizer\n\ncollate_fn = DataCollatorWithPadding(tokenizer=config['tokenizer'])","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:48:06.456183Z","iopub.execute_input":"2022-11-27T19:48:06.456483Z","iopub.status.idle":"2022-11-27T19:48:13.057667Z","shell.execute_reply.started":"2022-11-27T19:48:06.456440Z","shell.execute_reply":"2022-11-27T19:48:13.056488Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6266f5556b54121b0ffb168e8c243c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/580 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9870a7f818c14e98b686b19063eb0b7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03dbd80f2764419dbfaa28866d973a1d"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training Setup","metadata":{"execution":{"iopub.status.busy":"2022-11-25T00:52:07.410123Z","iopub.execute_input":"2022-11-25T00:52:07.410727Z","iopub.status.idle":"2022-11-25T00:52:07.415775Z","shell.execute_reply.started":"2022-11-25T00:52:07.410687Z","shell.execute_reply":"2022-11-25T00:52:07.414597Z"}}},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n        loss = loss_fn(outputs.logits, inputs['target'], loss_type=config['loss_type'])\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:48:13.058950Z","iopub.execute_input":"2022-11-27T19:48:13.059640Z","iopub.status.idle":"2022-11-27T19:48:13.065520Z","shell.execute_reply.started":"2022-11-27T19:48:13.059609Z","shell.execute_reply":"2022-11-27T19:48:13.064475Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for fold in range(1, config['n_fold']):\n    print(f\"========== Fold: {fold} ==========\")\n    \n    if not config[\"debug\"]:\n        run = wandb.init(project=config['competition'], \n                         config=config,\n                         job_type='Train',\n                         group=config['group'],\n                         tags=[config['model_name'], config['loss_type']],\n                         name=f'{config[\"hash_name\"]}-fold-{fold}',\n                         anonymous='must')\n\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    train_dataset = FeedBackDataset(df_train, tokenizer=config['tokenizer'], max_length=config['max_length'], target_label = config[\"target_cols\"])\n    valid_dataset = FeedBackDataset(df_valid, tokenizer=config['tokenizer'], max_length=config['max_length'], target_label = config[\"target_cols\"])\n\n    model = FeedBackModel(config['model_name'], config[\"num_classes\"], PoolingLayer = config[\"PoolingLayer\"]).to(config['device'])\n\n    # Define Optimizer and Scheduler\n    optimizer, scheduler = optimizer_setup(model=model, \n                                           config=config, \n                                           train_dataset_size =len(train_dataset),\n                                           layerwise = config[\"layerwise\"]\n                                          )\n\n    training_args = TrainingArguments(\n        output_dir=f\"/kaggle/tmp/outputs-{fold}/\",\n        evaluation_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        per_device_train_batch_size=config['train_batch_size'],\n        per_device_eval_batch_size=config['valid_batch_size'],\n        num_train_epochs= config['epochs'],\n        learning_rate= config['learning_rate'],\n        weight_decay= config['weight_decay'],\n        gradient_accumulation_steps=config['n_accumulate'],\n        max_grad_norm=config['max_grad_norm'],\n        seed=config['seed'],\n        fp16  = config[\"fp16_enable\"],\n        fp16_full_eval  = config[\"fp16_enable\"],\n        group_by_length = True,\n        half_precision_backend = \"cuda_amp\",\n        metric_for_best_model= 'eval_mcrmse',\n        load_best_model_at_end=True,\n        greater_is_better=False,\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        report_to = \"wandb\",\n        label_names = [\"target\"]\n    )\n\n\n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        data_collator=collate_fn,\n        optimizers=(optimizer, scheduler),\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    trainer.save_model()\n\n    #evaluation = trainer.evaluate()\n    #run.log({\"score_mcrmse\": evaluation[\"eval_mcrmse\"], \"eval_runtime\": evaluation[\"eval_runtime\"]})\n    if not config[\"debug\"]:\n        run.finish()\n\n    del model, train_dataset, valid_dataset\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:48:13.067140Z","iopub.execute_input":"2022-11-27T19:48:13.067865Z","iopub.status.idle":"2022-11-27T23:15:50.402425Z","shell.execute_reply.started":"2022-11-27T19:48:13.067828Z","shell.execute_reply":"2022-11-27T23:15:50.400538Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhazrulakmal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"========== Fold: 1 ==========\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20221127_194813-2j9rntj5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/hazrulakmal/FeedBack3/runs/2j9rntj5\" target=\"_blank\">defiant-pastel-millipede-fold-1</a></strong> to <a href=\"https://wandb.ai/hazrulakmal/FeedBack3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/833M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c139d1c9a2547fd8bc9485dd18c8b5f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nUsing cuda_amp half precision backend\n***** Running training *****\n  Num examples = 2932\n  Num Epochs = 5\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 4\n  Total optimization steps = 915\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [915/915 1:08:03, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mcrmse</th>\n      <th>Cohesion Rmse</th>\n      <th>Syntax Rmse</th>\n      <th>Vocabulary Rmse</th>\n      <th>Phraseology Rmse</th>\n      <th>Grammar Rmse</th>\n      <th>Conventions Rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.655500</td>\n      <td>0.119582</td>\n      <td>0.490918</td>\n      <td>0.533877</td>\n      <td>0.465856</td>\n      <td>0.478416</td>\n      <td>0.485549</td>\n      <td>0.514728</td>\n      <td>0.467083</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.139700</td>\n      <td>0.109250</td>\n      <td>0.468507</td>\n      <td>0.495006</td>\n      <td>0.451374</td>\n      <td>0.447736</td>\n      <td>0.452634</td>\n      <td>0.499617</td>\n      <td>0.464674</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.126400</td>\n      <td>0.123965</td>\n      <td>0.498721</td>\n      <td>0.600911</td>\n      <td>0.475318</td>\n      <td>0.470459</td>\n      <td>0.487780</td>\n      <td>0.495478</td>\n      <td>0.462377</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.118300</td>\n      <td>0.109836</td>\n      <td>0.469683</td>\n      <td>0.511261</td>\n      <td>0.456780</td>\n      <td>0.430009</td>\n      <td>0.468661</td>\n      <td>0.494167</td>\n      <td>0.457218</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.110200</td>\n      <td>0.105246</td>\n      <td>0.459600</td>\n      <td>0.486948</td>\n      <td>0.447524</td>\n      <td>0.423641</td>\n      <td>0.446978</td>\n      <td>0.493600</td>\n      <td>0.458912</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 979\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-1/checkpoint-183\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 979\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-1/checkpoint-366\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting older checkpoint [/kaggle/tmp/outputs-1/checkpoint-183] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 979\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-1/checkpoint-549\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 979\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-1/checkpoint-732\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting older checkpoint [/kaggle/tmp/outputs-1/checkpoint-549] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 979\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-1/checkpoint-915\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting older checkpoint [/kaggle/tmp/outputs-1/checkpoint-366] due to args.save_total_limit\nDeleting older checkpoint [/kaggle/tmp/outputs-1/checkpoint-732] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from /kaggle/tmp/outputs-1/checkpoint-915 (score: 0.459600427870575).\nSaving model checkpoint to /kaggle/tmp/outputs-1/\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/cohesion_rmse</td><td>▄▁█▂▁</td></tr><tr><td>eval/conventions_rmse</td><td>█▆▅▁▂</td></tr><tr><td>eval/grammar_rmse</td><td>█▃▂▁▁</td></tr><tr><td>eval/loss</td><td>▆▂█▃▁</td></tr><tr><td>eval/mcrmse</td><td>▇▃█▃▁</td></tr><tr><td>eval/phraseology_rmse</td><td>█▂█▅▁</td></tr><tr><td>eval/runtime</td><td>▂▁▁▆█</td></tr><tr><td>eval/samples_per_second</td><td>▇██▃▁</td></tr><tr><td>eval/steps_per_second</td><td>▇██▃▁</td></tr><tr><td>eval/syntax_rmse</td><td>▆▂█▃▁</td></tr><tr><td>eval/vocabulary_rmse</td><td>█▄▇▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/learning_rate</td><td>█▆▄▃▁</td></tr><tr><td>train/loss</td><td>█▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/cohesion_rmse</td><td>0.48695</td></tr><tr><td>eval/conventions_rmse</td><td>0.45891</td></tr><tr><td>eval/grammar_rmse</td><td>0.4936</td></tr><tr><td>eval/loss</td><td>0.10525</td></tr><tr><td>eval/mcrmse</td><td>0.4596</td></tr><tr><td>eval/phraseology_rmse</td><td>0.44698</td></tr><tr><td>eval/runtime</td><td>89.1817</td></tr><tr><td>eval/samples_per_second</td><td>10.978</td></tr><tr><td>eval/steps_per_second</td><td>1.379</td></tr><tr><td>eval/syntax_rmse</td><td>0.44752</td></tr><tr><td>eval/vocabulary_rmse</td><td>0.42364</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>915</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1102</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>0.23003</td></tr><tr><td>train/train_runtime</td><td>4088.8274</td></tr><tr><td>train/train_samples_per_second</td><td>3.585</td></tr><tr><td>train/train_steps_per_second</td><td>0.224</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced <strong style=\"color:#cdcd00\">defiant-pastel-millipede-fold-1</strong>: <a href=\"https://wandb.ai/hazrulakmal/FeedBack3/runs/2j9rntj5\" target=\"_blank\">https://wandb.ai/hazrulakmal/FeedBack3/runs/2j9rntj5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20221127_194813-2j9rntj5/logs</code>"},"metadata":{}},{"name":"stdout","text":"========== Fold: 2 ==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20221127_205743-2udaz06z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/hazrulakmal/FeedBack3/runs/2udaz06z\" target=\"_blank\">defiant-pastel-millipede-fold-2</a></strong> to <a href=\"https://wandb.ai/hazrulakmal/FeedBack3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"name":"stderr","text":"loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/eed737dd80585a756b0286a093059c2b4403b98a17ac2cb50cda7799c653fc11.e38140a56995392eade33ad2835bb905412b65ba305475bd577c00edb10c45d9\nSome weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at microsoft/deberta-v3-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\nPyTorch: setting up devices\nUsing cuda_amp half precision backend\n***** Running training *****\n  Num examples = 2939\n  Num Epochs = 5\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 4\n  Total optimization steps = 915\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [915/915 1:08:32, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mcrmse</th>\n      <th>Cohesion Rmse</th>\n      <th>Syntax Rmse</th>\n      <th>Vocabulary Rmse</th>\n      <th>Phraseology Rmse</th>\n      <th>Grammar Rmse</th>\n      <th>Conventions Rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.624000</td>\n      <td>0.119669</td>\n      <td>0.489521</td>\n      <td>0.571417</td>\n      <td>0.473922</td>\n      <td>0.445101</td>\n      <td>0.477151</td>\n      <td>0.497821</td>\n      <td>0.471713</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.142400</td>\n      <td>0.115761</td>\n      <td>0.481797</td>\n      <td>0.537880</td>\n      <td>0.472389</td>\n      <td>0.451559</td>\n      <td>0.476061</td>\n      <td>0.500974</td>\n      <td>0.451918</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.130600</td>\n      <td>0.120265</td>\n      <td>0.491158</td>\n      <td>0.553021</td>\n      <td>0.470494</td>\n      <td>0.486130</td>\n      <td>0.501593</td>\n      <td>0.489833</td>\n      <td>0.445877</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.122600</td>\n      <td>0.113088</td>\n      <td>0.476043</td>\n      <td>0.526573</td>\n      <td>0.466251</td>\n      <td>0.451297</td>\n      <td>0.475307</td>\n      <td>0.490174</td>\n      <td>0.446659</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.114200</td>\n      <td>0.107469</td>\n      <td>0.464011</td>\n      <td>0.494900</td>\n      <td>0.462923</td>\n      <td>0.421621</td>\n      <td>0.465610</td>\n      <td>0.495680</td>\n      <td>0.443333</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-2/checkpoint-183\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-2/checkpoint-366\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting older checkpoint [/kaggle/tmp/outputs-2/checkpoint-183] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-2/checkpoint-549\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-2/checkpoint-732\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting older checkpoint [/kaggle/tmp/outputs-2/checkpoint-366] due to args.save_total_limit\nDeleting older checkpoint [/kaggle/tmp/outputs-2/checkpoint-549] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-2/checkpoint-915\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting older checkpoint [/kaggle/tmp/outputs-2/checkpoint-732] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from /kaggle/tmp/outputs-2/checkpoint-915 (score: 0.4640113140587243).\nSaving model checkpoint to /kaggle/tmp/outputs-2/\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/cohesion_rmse</td><td>█▅▆▄▁</td></tr><tr><td>eval/conventions_rmse</td><td>█▃▂▂▁</td></tr><tr><td>eval/grammar_rmse</td><td>▆█▁▁▅</td></tr><tr><td>eval/loss</td><td>█▆█▄▁</td></tr><tr><td>eval/mcrmse</td><td>█▆█▄▁</td></tr><tr><td>eval/phraseology_rmse</td><td>▃▃█▃▁</td></tr><tr><td>eval/runtime</td><td>▂█▁▄▃</td></tr><tr><td>eval/samples_per_second</td><td>▇▁█▅▆</td></tr><tr><td>eval/steps_per_second</td><td>█▁█▅▇</td></tr><tr><td>eval/syntax_rmse</td><td>█▇▆▃▁</td></tr><tr><td>eval/vocabulary_rmse</td><td>▄▄█▄▁</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/learning_rate</td><td>█▆▄▃▁</td></tr><tr><td>train/loss</td><td>█▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/cohesion_rmse</td><td>0.4949</td></tr><tr><td>eval/conventions_rmse</td><td>0.44333</td></tr><tr><td>eval/grammar_rmse</td><td>0.49568</td></tr><tr><td>eval/loss</td><td>0.10747</td></tr><tr><td>eval/mcrmse</td><td>0.46401</td></tr><tr><td>eval/phraseology_rmse</td><td>0.46561</td></tr><tr><td>eval/runtime</td><td>87.8416</td></tr><tr><td>eval/samples_per_second</td><td>11.065</td></tr><tr><td>eval/steps_per_second</td><td>1.389</td></tr><tr><td>eval/syntax_rmse</td><td>0.46292</td></tr><tr><td>eval/vocabulary_rmse</td><td>0.42162</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>915</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1142</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>0.22676</td></tr><tr><td>train/train_runtime</td><td>4118.7334</td></tr><tr><td>train/train_samples_per_second</td><td>3.568</td></tr><tr><td>train/train_steps_per_second</td><td>0.222</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced <strong style=\"color:#cdcd00\">defiant-pastel-millipede-fold-2</strong>: <a href=\"https://wandb.ai/hazrulakmal/FeedBack3/runs/2udaz06z\" target=\"_blank\">https://wandb.ai/hazrulakmal/FeedBack3/runs/2udaz06z</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20221127_205743-2udaz06z/logs</code>"},"metadata":{}},{"name":"stdout","text":"========== Fold: 3 ==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20221127_220704-2kmw1xr1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/hazrulakmal/FeedBack3/runs/2kmw1xr1\" target=\"_blank\">defiant-pastel-millipede-fold-3</a></strong> to <a href=\"https://wandb.ai/hazrulakmal/FeedBack3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"name":"stderr","text":"loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/eed737dd80585a756b0286a093059c2b4403b98a17ac2cb50cda7799c653fc11.e38140a56995392eade33ad2835bb905412b65ba305475bd577c00edb10c45d9\nSome weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at microsoft/deberta-v3-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\nPyTorch: setting up devices\nUsing cuda_amp half precision backend\n***** Running training *****\n  Num examples = 2930\n  Num Epochs = 5\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 4\n  Total optimization steps = 915\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [915/915 1:08:02, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Mcrmse</th>\n      <th>Cohesion Rmse</th>\n      <th>Syntax Rmse</th>\n      <th>Vocabulary Rmse</th>\n      <th>Phraseology Rmse</th>\n      <th>Grammar Rmse</th>\n      <th>Conventions Rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.607200</td>\n      <td>0.119475</td>\n      <td>0.490002</td>\n      <td>0.557049</td>\n      <td>0.444237</td>\n      <td>0.480255</td>\n      <td>0.512704</td>\n      <td>0.487765</td>\n      <td>0.458005</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.141900</td>\n      <td>0.115350</td>\n      <td>0.480909</td>\n      <td>0.561097</td>\n      <td>0.447162</td>\n      <td>0.452858</td>\n      <td>0.501808</td>\n      <td>0.477023</td>\n      <td>0.445506</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.125200</td>\n      <td>0.107359</td>\n      <td>0.464495</td>\n      <td>0.503046</td>\n      <td>0.437213</td>\n      <td>0.448171</td>\n      <td>0.469821</td>\n      <td>0.483411</td>\n      <td>0.445308</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.115800</td>\n      <td>0.108978</td>\n      <td>0.468103</td>\n      <td>0.496291</td>\n      <td>0.442291</td>\n      <td>0.459248</td>\n      <td>0.474233</td>\n      <td>0.487118</td>\n      <td>0.449439</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.105400</td>\n      <td>0.106425</td>\n      <td>0.462756</td>\n      <td>0.483304</td>\n      <td>0.442764</td>\n      <td>0.440090</td>\n      <td>0.475292</td>\n      <td>0.484292</td>\n      <td>0.450794</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 981\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-3/checkpoint-183\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 981\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-3/checkpoint-366\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting older checkpoint [/kaggle/tmp/outputs-3/checkpoint-183] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 981\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-3/checkpoint-549\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting older checkpoint [/kaggle/tmp/outputs-3/checkpoint-366] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 981\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-3/checkpoint-732\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n***** Running Evaluation *****\n  Num examples = 981\n  Batch size = 8\nSaving model checkpoint to /kaggle/tmp/outputs-3/checkpoint-915\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\nDeleting older checkpoint [/kaggle/tmp/outputs-3/checkpoint-549] due to args.save_total_limit\nDeleting older checkpoint [/kaggle/tmp/outputs-3/checkpoint-732] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from /kaggle/tmp/outputs-3/checkpoint-915 (score: 0.4627560060023373).\nSaving model checkpoint to /kaggle/tmp/outputs-3/\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/cohesion_rmse</td><td>██▃▂▁</td></tr><tr><td>eval/conventions_rmse</td><td>█▁▁▃▄</td></tr><tr><td>eval/grammar_rmse</td><td>█▁▅█▆</td></tr><tr><td>eval/loss</td><td>█▆▂▂▁</td></tr><tr><td>eval/mcrmse</td><td>█▆▁▂▁</td></tr><tr><td>eval/phraseology_rmse</td><td>█▆▁▂▂</td></tr><tr><td>eval/runtime</td><td>█▁▂▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▁█▇█▇</td></tr><tr><td>eval/steps_per_second</td><td>▁█▆█▆</td></tr><tr><td>eval/syntax_rmse</td><td>▆█▁▅▅</td></tr><tr><td>eval/vocabulary_rmse</td><td>█▃▂▄▁</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/learning_rate</td><td>█▆▅▃▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/cohesion_rmse</td><td>0.4833</td></tr><tr><td>eval/conventions_rmse</td><td>0.45079</td></tr><tr><td>eval/grammar_rmse</td><td>0.48429</td></tr><tr><td>eval/loss</td><td>0.10643</td></tr><tr><td>eval/mcrmse</td><td>0.46276</td></tr><tr><td>eval/phraseology_rmse</td><td>0.47529</td></tr><tr><td>eval/runtime</td><td>88.3982</td></tr><tr><td>eval/samples_per_second</td><td>11.098</td></tr><tr><td>eval/steps_per_second</td><td>1.391</td></tr><tr><td>eval/syntax_rmse</td><td>0.44276</td></tr><tr><td>eval/vocabulary_rmse</td><td>0.44009</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>915</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1054</td></tr><tr><td>train/total_flos</td><td>0.0</td></tr><tr><td>train/train_loss</td><td>0.21912</td></tr><tr><td>train/train_runtime</td><td>4087.7006</td></tr><tr><td>train/train_samples_per_second</td><td>3.584</td></tr><tr><td>train/train_steps_per_second</td><td>0.224</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced <strong style=\"color:#cdcd00\">defiant-pastel-millipede-fold-3</strong>: <a href=\"https://wandb.ai/hazrulakmal/FeedBack3/runs/2kmw1xr1\" target=\"_blank\">https://wandb.ai/hazrulakmal/FeedBack3/runs/2kmw1xr1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20221127_220704-2kmw1xr1/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"from pathlib import Path\nTMP_DIR = Path('../temp')\nTMP_DIR.mkdir(exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:31:18.201254Z","iopub.execute_input":"2022-11-27T23:31:18.201682Z","iopub.status.idle":"2022-11-27T23:31:18.208301Z","shell.execute_reply.started":"2022-11-27T23:31:18.201648Z","shell.execute_reply":"2022-11-27T23:31:18.206796Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"ls ../","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:51:58.244382Z","iopub.execute_input":"2022-11-27T23:51:58.245385Z","iopub.status.idle":"2022-11-27T23:51:59.333811Z","shell.execute_reply.started":"2022-11-27T23:51:58.245347Z","shell.execute_reply":"2022-11-27T23:51:59.332588Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[0m\u001b[01;34moutputs-1\u001b[0m/  \u001b[01;34moutputs-2\u001b[0m/  \u001b[01;34moutputs-3\u001b[0m/  \u001b[01;34mtemp\u001b[0m/\n","output_type":"stream"}]},{"cell_type":"code","source":"ls ../outputs-1","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:32:16.851743Z","iopub.execute_input":"2022-11-27T23:32:16.852187Z","iopub.status.idle":"2022-11-27T23:32:17.918644Z","shell.execute_reply.started":"2022-11-27T23:32:16.852155Z","shell.execute_reply":"2022-11-27T23:32:17.917277Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[0m\u001b[01;34mcheckpoint-915\u001b[0m/  output-1.zip  pytorch_model.bin  training_args.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:38:42.720030Z","iopub.execute_input":"2022-11-27T23:38:42.720398Z","iopub.status.idle":"2022-11-27T23:38:42.725645Z","shell.execute_reply.started":"2022-11-27T23:38:42.720368Z","shell.execute_reply":"2022-11-27T23:38:42.724298Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:56:54.780753Z","iopub.execute_input":"2022-11-27T23:56:54.781194Z","iopub.status.idle":"2022-11-27T23:56:59.458692Z","shell.execute_reply.started":"2022-11-27T23:56:54.781155Z","shell.execute_reply":"2022-11-27T23:56:59.456931Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nSaving model checkpoint to /kaggle/working/outputs-1\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n","output_type":"stream"}]},{"cell_type":"code","source":"path = \"../outputs-3/pytorch_model.bin\"\nmodel = FeedBackModel(config['model_name'], config[\"num_classes\"], PoolingLayer = config[\"PoolingLayer\"]).to(config['device'])\nmodel.load_state_dict(torch.load(path, map_location= config[\"device\"]))","metadata":{"execution":{"iopub.status.busy":"2022-11-28T00:01:17.291920Z","iopub.execute_input":"2022-11-28T00:01:17.292305Z","iopub.status.idle":"2022-11-28T00:01:49.430786Z","shell.execute_reply.started":"2022-11-28T00:01:17.292273Z","shell.execute_reply":"2022-11-28T00:01:49.429669Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/eed737dd80585a756b0286a093059c2b4403b98a17ac2cb50cda7799c653fc11.e38140a56995392eade33ad2835bb905412b65ba305475bd577c00edb10c45d9\nSome weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at microsoft/deberta-v3-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f\"/kaggle/working/outputs-{3}\")\n\ntrainer = CustomTrainer(\n        model=model,\n        args=training_args)\n\ntrainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2022-11-28T00:01:49.433037Z","iopub.execute_input":"2022-11-28T00:01:49.433359Z","iopub.status.idle":"2022-11-28T00:01:54.149770Z","shell.execute_reply.started":"2022-11-28T00:01:49.433330Z","shell.execute_reply":"2022-11-28T00:01:54.147695Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nSaving model checkpoint to /kaggle/working/outputs-3\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\n","output_type":"stream"}]},{"cell_type":"code","source":"path = f\"{config['model_name'].replace('/', '-')}_fold_1_best.pth\"","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:51:13.412647Z","iopub.execute_input":"2022-11-27T23:51:13.413025Z","iopub.status.idle":"2022-11-27T23:51:13.418269Z","shell.execute_reply.started":"2022-11-27T23:51:13.412996Z","shell.execute_reply":"2022-11-27T23:51:13.417172Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"./\" + path )","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:53:54.722153Z","iopub.execute_input":"2022-11-27T23:53:54.722646Z","iopub.status.idle":"2022-11-27T23:54:03.059402Z","shell.execute_reply.started":"2022-11-27T23:53:54.722610Z","shell.execute_reply":"2022-11-27T23:54:03.058340Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nimport zipfile","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:25:14.104208Z","iopub.execute_input":"2022-11-27T23:25:14.104718Z","iopub.status.idle":"2022-11-27T23:25:14.111269Z","shell.execute_reply.started":"2022-11-27T23:25:14.104685Z","shell.execute_reply":"2022-11-27T23:25:14.110227Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"FileLink(r'/kaggle/working/outputs-2/pytorch_model.bin')","metadata":{"execution":{"iopub.status.busy":"2022-11-28T00:01:05.182397Z","iopub.execute_input":"2022-11-28T00:01:05.183036Z","iopub.status.idle":"2022-11-28T00:01:05.189817Z","shell.execute_reply.started":"2022-11-28T00:01:05.182999Z","shell.execute_reply":"2022-11-28T00:01:05.188884Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/outputs-2/pytorch_model.bin","text/html":"<a href='/kaggle/working/outputs-2/pytorch_model.bin' target='_blank'>/kaggle/working/outputs-2/pytorch_model.bin</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"def zip_dir(directory = os.curdir, file_name = 'output-1.zip'):\n    \"\"\"\n    zip all the files in a directory\n    \n    Parameters\n    _____\n    directory: str\n        directory needs to be zipped, defualt is current working directory\n        \n    file_name: str\n        the name of the zipped file (including .zip), default is 'directory.zip'\n        \n    Returns\n    _____\n    Creates a hyperlink, which can be used to download the zip file)\n    \"\"\"\n    os.chdir(directory)\n    zip_ref = zipfile.ZipFile(file_name, mode='w')\n    for folder, _, files in os.walk(directory):\n        for file in files:\n            if file_name in file:\n                pass\n            else:\n                zip_ref.write(os.path.join(folder, file))\n\n    return FileLink(file_name)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:24:31.480225Z","iopub.execute_input":"2022-11-27T23:24:31.481007Z","iopub.status.idle":"2022-11-27T23:24:31.488816Z","shell.execute_reply.started":"2022-11-27T23:24:31.480967Z","shell.execute_reply":"2022-11-27T23:24:31.487335Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"zip_dir('../outputs-1')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:33:39.529120Z","iopub.execute_input":"2022-11-27T23:33:39.529595Z","iopub.status.idle":"2022-11-27T23:34:39.172507Z","shell.execute_reply.started":"2022-11-27T23:33:39.529561Z","shell.execute_reply":"2022-11-27T23:34:39.171501Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"/kaggle/tmp/outputs-1/output-1.zip","text/html":"<a href='output-1.zip' target='_blank'>output-1.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"zip_dir('/kaggle/tmp/outputs-2', file_name='output-2.zip')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T23:28:50.676372Z","iopub.execute_input":"2022-11-27T23:28:50.677395Z","iopub.status.idle":"2022-11-27T23:29:44.713390Z","shell.execute_reply.started":"2022-11-27T23:28:50.677357Z","shell.execute_reply":"2022-11-27T23:29:44.711248Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"/kaggle/tmp/outputs-2/output-2.zip","text/html":"<a href='output-2.zip' target='_blank'>output-2.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"zip_dir('/kaggle/tmp/outputs-3', file_name='output-3.zip')","metadata":{},"execution_count":null,"outputs":[]}]}