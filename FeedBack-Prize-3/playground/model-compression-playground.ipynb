{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pynvml","metadata":{"execution":{"iopub.status.busy":"2022-11-28T19:56:05.240721Z","iopub.execute_input":"2022-11-28T19:56:05.241212Z","iopub.status.idle":"2022-11-28T19:56:16.759974Z","shell.execute_reply.started":"2022-11-28T19:56:05.241172Z","shell.execute_reply":"2022-11-28T19:56:16.758737Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pynvml in /opt/conda/lib/python3.7/site-packages (11.4.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import gc\nimport os\nfrom pynvml import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom transformers import DataCollatorWithPadding\nfrom transformers import Trainer, TrainingArguments\n\nfrom tqdm import tqdm\nfrom feedback_custom_funtions import loss_fn, optimizer_setup, FeedBackDataset, RMSELoss, compute_metrics\nfrom model_building import MeanPooling, MaxPooling, MinPooling, AttentionPooling, FeedBackModel\n\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n    \ndef print_summary(result):\n    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n    print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2022-11-28T19:56:16.762607Z","iopub.execute_input":"2022-11-28T19:56:16.762912Z","iopub.status.idle":"2022-11-28T19:56:26.304049Z","shell.execute_reply.started":"2022-11-28T19:56:16.762883Z","shell.execute_reply":"2022-11-28T19:56:26.302938Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"config = {\"seed\": 42,\n          \"epochs\": 1,\n          \"debug\" : True,\n          \"model_name\": \"microsoft/deberta-v3-large\",\n          \"PoolingLayer\": AttentionPooling(1024),\n          \"group\" : \"deberta-v3-Large-AP-LLRD\" ,\n          \"loss_type\": \"smooth_l1\", # ['mse', 'rmse', 'smooth_l1']\n          \"train_batch_size\": 4,\n          \"valid_batch_size\": 8,\n          \"fp16_enable\" : True,\n          \"max_length\": 512,\n          \"layerwise\" : True,\n          \"learning_rate\": 1e-5,\n          \"decoder_lr\": 1e-4,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 4,\n          \"n_accumulate\": 4,\n          \"max_grad_norm\": 1000,\n          \"num_classes\": 6,\n          \"target_cols\": [\"cohesion\", \"syntax\", \"vocabulary\", \n                          \"phraseology\", \"grammar\", \"conventions\"],\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          \"competition\": \"FeedBack3\",\n          \"_wandb_kernel\": \"hazrul\",\n          \"check_model_gpu_usage\": False\n          }","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:25:52.983561Z","iopub.execute_input":"2022-11-27T19:25:52.984673Z","iopub.status.idle":"2022-11-27T19:25:53.063550Z","shell.execute_reply.started":"2022-11-27T19:25:52.984636Z","shell.execute_reply":"2022-11-27T19:25:53.062586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config[\"check_model_gpu_usage\"]: \n    config_model = AutoConfig.from_pretrained(\"microsoft/deberta-xlarge\")\n    model = FeedBackModel(\"microsoft/deberta-xlarge\", 6, AttentionPooling(config_model.hidden_size)).to(config[\"device\"])\n    print_gpu_utilization()\n\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:25:53.066308Z","iopub.execute_input":"2022-11-27T19:25:53.066691Z","iopub.status.idle":"2022-11-27T19:25:53.072524Z","shell.execute_reply.started":"2022-11-27T19:25:53.066655Z","shell.execute_reply":"2022-11-27T19:25:53.071510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FeedBackModel Architecture GPU usage\n\n1. Large : GPU memory occupied: 2321 MB\n2. Base : GPU memory occupied: 1403 MB\n3. Small : GPU memory occupied: 1223 MB","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/feedbackprizemultilabelstratifiedkfold/kfold_train_FB_comptetion.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:25:53.074248Z","iopub.execute_input":"2022-11-27T19:25:53.074948Z","iopub.status.idle":"2022-11-27T19:25:53.279181Z","shell.execute_reply.started":"2022-11-27T19:25:53.074913Z","shell.execute_reply":"2022-11-27T19:25:53.278087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\nconfig[\"tokenizer\"] = tokenizer\ncollate_fn = DataCollatorWithPadding(tokenizer=config['tokenizer'])","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:25:53.280757Z","iopub.execute_input":"2022-11-27T19:25:53.281121Z","iopub.status.idle":"2022-11-27T19:25:59.932943Z","shell.execute_reply.started":"2022-11-27T19:25:53.281081Z","shell.execute_reply":"2022-11-27T19:25:59.931850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n        loss = loss_fn(outputs.logits, inputs['target'], loss_type=config['loss_type'])\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:25:59.935668Z","iopub.execute_input":"2022-11-27T19:25:59.936308Z","iopub.status.idle":"2022-11-27T19:25:59.942344Z","shell.execute_reply.started":"2022-11-27T19:25:59.936270Z","shell.execute_reply":"2022-11-27T19:25:59.940684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df[df.kfold != 1].reset_index(drop=True)\ndf_valid = df[df.kfold == 1].reset_index(drop=True)\n\ntrain_dataset = FeedBackDataset(df_train, tokenizer=config['tokenizer'], max_length=config['max_length'], target_label = config[\"target_cols\"])\nvalid_dataset = FeedBackDataset(df_valid, tokenizer=config['tokenizer'], max_length=config['max_length'], target_label = config[\"target_cols\"])\n\nmodel = FeedBackModel(config['model_name'], config[\"num_classes\"], PoolingLayer = config[\"PoolingLayer\"]).to(config['device'])\n\n# Define Optimizer and Scheduler\noptimizer, scheduler = optimizer_setup(model=model, \n                                       config=config, \n                                       train_dataset_size =len(train_dataset),\n                                       layerwise = config[\"layerwise\"]\n                                      )\n\ntraining_args = TrainingArguments(\n    output_dir=f\"outputs\",\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    per_device_train_batch_size=config['train_batch_size'],\n    per_device_eval_batch_size=config['valid_batch_size'],\n    num_train_epochs= config['epochs'],\n    learning_rate= config['learning_rate'],\n    weight_decay= config['weight_decay'],\n    gradient_accumulation_steps=config['n_accumulate'],\n    max_grad_norm=config['max_grad_norm'],\n    seed=config['seed'],\n    fp16  = config[\"fp16_enable\"],\n    fp16_full_eval  =config[\"fp16_enable\"],\n    group_by_length = True,\n    metric_for_best_model= 'eval_mcrmse',\n    load_best_model_at_end=True,\n    greater_is_better=False,\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    report_to = None, #\"wandb\",\n    label_names = [\"target\"]\n)\n\n\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    data_collator=collate_fn,\n    optimizers=(optimizer, scheduler),\n    compute_metrics=compute_metrics\n)\n\nresult = trainer.train()\nprint_summary(result)\n\ndel model, train_dataset, valid_dataset\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:25:59.944741Z","iopub.execute_input":"2022-11-27T19:25:59.945164Z","iopub.status.idle":"2022-11-27T19:40:36.585472Z","shell.execute_reply.started":"2022-11-27T19:25:59.945129Z","shell.execute_reply":"2022-11-27T19:40:36.584509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Training & eval batch size : 2, gradient accumulation 8, fp16 True, total batch size 16\n    - Time: 1008.12\n    - Samples/second: 2.91\n    - GPU memory occupied: 11591 MB.\n    \n2. Training batch size: 4,  eval batch size : 3, gradient accumulation 8, fp16 True, total batch size 32\n    - Time: 829.84\n    - Samples/second: 3.54\n    - GPU memory occupied: 14853 MB.\n    - Score >0.5\n    \n3. Training batch size 8, eval batch size 8, gradient accumulation 2, fp16 True, total batch size 16\n    - Out of CUDA\n    \n4. Training batch size: 4,  eval batch size : 4, gradient accumulation 4, fp16 True, total batch size 24\n    - Time: 826.07\n    - Samples/second: 3.55\n    - GPU memory occupied: 14853 MB.\n    - Score: 0.48364236942074873\n5. Training 4, eval 8 gradient acumulation 4, fp16 True, total batch size 24\n\n    - Time: 819.41\n    - Samples/second: 3.58\n    - GPU memory occupied: 14851 MB.\n    - 0.4805801246195733\n    \n## Base\n1. Trainin 8 Eval 8 GA 4 batch size 32\n    - Time: 242.02\n    - Samples/second: 12.12\n    - GPU memory occupied: 10173 MB.\n    \n2. Trainin 8 Eval 16 GA 4 batch size 32\n    - Time: 237.08\n    - Samples/second: 12.37\n    - GPU memory occupied: 10941 MB.\n    - 0.5631028740872916","metadata":{}},{"cell_type":"code","source":"config = dict(\n    seed = 42,\n    num_models = 3,\n    model_name_0 = '../input/debertav3base',\n    model_name_1 = \"../input/deberta-v3-large/deberta-v3-large\",\n    model_name_2 = \"../input/roberta-base\",\n    model_name = \"microsoft/deberta-v3-large\",\n    test_batch_size = 64,\n    max_length = 512,\n    num_classes = 6,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    pooling_1 = AttentionPooling(1024),\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T20:02:58.262191Z","iopub.execute_input":"2022-11-28T20:02:58.263185Z","iopub.status.idle":"2022-11-28T20:02:58.279337Z","shell.execute_reply.started":"2022-11-28T20:02:58.263133Z","shell.execute_reply":"2022-11-28T20:02:58.278326Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\nconfig[\"tokenizer\"] = tokenizer\ncollate_fn = DataCollatorWithPadding(tokenizer=config['tokenizer'])","metadata":{"execution":{"iopub.status.busy":"2022-11-28T20:03:44.282572Z","iopub.execute_input":"2022-11-28T20:03:44.282956Z","iopub.status.idle":"2022-11-28T20:03:48.582517Z","shell.execute_reply.started":"2022-11-28T20:03:44.282924Z","shell.execute_reply":"2022-11-28T20:03:48.581340Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/6386fc34376768db39488179803c16268ff12ee177a43a993690f66b7d7a0b7c.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/cae8294cb38511dc11086c090549f0a079bc5537a0f9a482d8358f17acc8cff0.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\nloading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/feedbackprizemultilabelstratifiedkfold/kfold_train_FB_comptetion.csv\")\ntest_df = df[df.kfold != 1].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T19:57:07.017399Z","iopub.execute_input":"2022-11-28T19:57:07.018059Z","iopub.status.idle":"2022-11-28T19:57:07.235998Z","shell.execute_reply.started":"2022-11-28T19:57:07.018018Z","shell.execute_reply":"2022-11-28T19:57:07.235062Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = FeedBackModel(config[f\"model_name\"], config[\"num_classes\"], config[f\"pooling_1\"])\nmodel.to(config['device'])           \n\ncollate_fn = DataCollatorWithPadding(tokenizer=config[f'tokenizer'])\ntest_dataset = FeedBackDataset(df=test_df, \n                               tokenizer=config[f'tokenizer'], \n                               max_length =config[\"max_length\"], \n                               train_mode=False)\n\n\ntraining_args = TrainingArguments(\n        output_dir=\".\",\n        per_device_eval_batch_size=config['test_batch_size'],\n        label_names=[\"target\"]\n    )\n\ntrainer = Trainer(model=model,\n                  args=training_args,\n                  data_collator=collate_fn)\n\npredictions = trainer.predict(test_dataset)                                                                       \nprint_gpu_utilization()\n\ndel model, test_dataset\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-28T20:03:51.954846Z","iopub.execute_input":"2022-11-28T20:03:51.955676Z","iopub.status.idle":"2022-11-28T20:07:46.569677Z","shell.execute_reply.started":"2022-11-28T20:03:51.955632Z","shell.execute_reply":"2022-11-28T20:07:46.568693Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/eed737dd80585a756b0286a093059c2b4403b98a17ac2cb50cda7799c653fc11.e38140a56995392eade33ad2835bb905412b65ba305475bd577c00edb10c45d9\nSome weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at microsoft/deberta-v3-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n***** Running Prediction *****\n  Num examples = 2932\n  Batch size = 64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [46/46 03:43]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"GPU memory occupied: 13607 MB.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"825"},"metadata":{}}]},{"cell_type":"code","source":"predictions.metrics","metadata":{"execution":{"iopub.status.busy":"2022-11-28T20:08:18.522486Z","iopub.execute_input":"2022-11-28T20:08:18.523291Z","iopub.status.idle":"2022-11-28T20:08:18.536407Z","shell.execute_reply.started":"2022-11-28T20:08:18.523225Z","shell.execute_reply":"2022-11-28T20:08:18.535299Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'test_runtime': 228.6626,\n 'test_samples_per_second': 12.822,\n 'test_steps_per_second': 0.201}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"32 - GPU memory occupied: 7471 MB.\n64 - GPU memory occupied: 13607 MB\n{'test_runtime': 228.6626,\n 'test_samples_per_second': 12.822,\n 'test_steps_per_second': 0.201}","metadata":{}}]}